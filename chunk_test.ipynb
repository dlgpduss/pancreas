{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/new_okay/lib/python3.9/site-packages/langchain_community/document_loaders/parsers/pdf.py:300: UserWarning: Warning: Empty content on page 0 of document ./췌장암_pdf/200207 국민암예방수칙 실천지침_췌장암.pdf\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/new_okay/lib/python3.9/site-packages/langchain_community/document_loaders/parsers/pdf.py:300: UserWarning: Warning: Empty content on page 28 of document ./췌장암_pdf/췌장암_대한종양내과학회.pdf\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 된 HTML파일 수: 42\n",
      "Results saved to 'chunk_size_overlap_tests.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import os\n",
    "import pandas as pd\n",
    "from charset_normalizer import detect\n",
    "\n",
    "# Step 1: Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Step 2: Test parameters\n",
    "chunk_sizes = [500, 1000, 1500]\n",
    "chunk_overlaps = [50, 100, 150, 200]\n",
    "results = []\n",
    "\n",
    "# Step 3: PDF and HTML file loading\n",
    "def load_documents():\n",
    "    pdf_docs = []\n",
    "    pdfs = glob('./췌장암_pdf/*.pdf')\n",
    "    for pdf in pdfs:\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        try:\n",
    "            documents = loader.load()\n",
    "            non_empty_docs = [doc for doc in documents if doc.page_content.strip()]\n",
    "            pdf_docs.extend(non_empty_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {pdf}: {e}\")\n",
    "    \n",
    "    html_docs = []\n",
    "    success_count = 0\n",
    "    html_files = glob('./췌장암_html/*.html')\n",
    "\n",
    "    for html_file in html_files:\n",
    "        try:\n",
    "            # 인코딩 자동 감지\n",
    "            with open(html_file, 'rb') as f:\n",
    "                raw_data = f.read()\n",
    "                detected_encoding = detect(raw_data)['encoding']\n",
    "\n",
    "            # 감지된 인코딩으로 파일 로드\n",
    "            loader = BSHTMLLoader(html_file, open_encoding=detected_encoding)\n",
    "            documents = loader.load()\n",
    "            success_count += 1\n",
    "            html_docs.extend(documents)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading HTML {html_file}: {e}\")\n",
    "\n",
    "    print(f'인코딩 된 HTML파일 수: {success_count}')\n",
    "    \n",
    "    return pdf_docs, html_docs\n",
    "\n",
    "pdf_docs, html_docs = load_documents()\n",
    "\n",
    "# Step 4: Test different chunk sizes and overlaps\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    for chunk_overlap in chunk_overlaps:\n",
    "        # Initialize the text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        # Split the documents\n",
    "        pdf_splits = text_splitter.split_documents(pdf_docs)\n",
    "        html_splits = text_splitter.split_documents(html_docs)\n",
    "        \n",
    "        # Combine and persist documents into Chroma\n",
    "        vectordb = Chroma(embedding_function=embedding, collection_name=f\"pancreas_{chunk_size}_{chunk_overlap}\")\n",
    "        vectordb.add_documents(pdf_splits)\n",
    "        vectordb.add_documents(html_splits)\n",
    "        \n",
    "        # Setup the retriever\n",
    "        retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "            retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            llm=llm\n",
    "        )\n",
    "        \n",
    "        # Setup the chain\n",
    "        template = \"\"\"\n",
    "        Your response should be in JSON format.\n",
    "        You are an empathetic chatbot designed to provide information and support regarding diseases.\n",
    "        You aim to maintain a consistent and friendly style while ensuring that explanations are easy for anyone to understand.\n",
    "        When explaining your symptoms and how to deal with them, please answer clearly and accurately. Depending on the situation, use jargon to explain, but don't lie, be precise and detailed.\n",
    "        You will do best to provide appropriate answers to questions.\n",
    "        Your goal is to provide the answers I seek and offer the assistance.\n",
    "        At the end of each response, You'll provide key terms related to the question, including diseases and medications.\n",
    "        Put a line break after the period.\n",
    "        Please refrain from unnecessary words.\n",
    "        Please answer me only once.\n",
    "        don't say it over and over again\n",
    "        Don't use repeated phrases.\n",
    "        Don't rewrite the question at the end.\n",
    "        Don't answer anything after writing the keywords.\n",
    "        Don't add questions at the end.\n",
    "        Stop answering after listing keyword words.\n",
    "        Please answer in 1000 words or less.\n",
    "        Please must reply in Korean.\n",
    "        you are designed to output json.\n",
    "\n",
    "\n",
    "        Answer the question based only on the following context: {context}\n",
    "\n",
    "        Question: {input}\n",
    "        Output Format (JSON)\n",
    "        {{\n",
    "        \"question\": \"Write the original user-submitted questions.\",\n",
    "        \"answer\": \"Full response to original question.\",\n",
    "        \"sources\": \"When writing your answer, organize it into sentences and record the context in which you wrote it.\",\n",
    "        \"source_documents\": [\n",
    "            {{\n",
    "            \"title\": \"The title of the source document.\",\n",
    "            \"page\": \"The page number of the source document.\",\n",
    "            \"content\": \"The relevant content extracted from the source document.\",\n",
    "            \"url\": \"The URL or file path of the source document.\"\n",
    "            }}]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        setup_and_retrieval = RunnableParallel(\n",
    "            {\"context\": retriever_from_llm, \"input\": lambda x: x}\n",
    "        )\n",
    "        chain = setup_and_retrieval | prompt | llm | StrOutputParser()\n",
    "        \n",
    "        # Test queries\n",
    "        queries = [\n",
    "            \"췌장암의 원인은 무엇인가요?\",\n",
    "            \"췌장암 1기와 2기 3기 차이를 알려주세요\",\n",
    "            \"췌장암이 유전과 관련이 있을 가능성이 있나요? 가족들에게 유전자 검사를 권해야 할까요?\",\n",
    "            \"췌장암 치료를 위해 현재 연구 중인 새로운 약물이나 임상시험에 참여할 수 있는 방법이 있나요?\",\n",
    "            \"췌장암으로 인해 발생하는 통증이나 소화 문제를 완화하기 위해 사용할 수 있는 방법이 무엇인가요?\",\n",
    "            \"췌장암이 당뇨병과 관련이 있다는 이야기를 들었는데, 제 혈당 관리가 암 치료에 어떤 영향을 미칠 수 있나요?\",\n",
    "            \"치료 과정 중에도 일상생활을 최대한 유지하고 생활의 질을 높이기 위해 추천할 만한 활동이나 프로그램이 있을까요?\",\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            try:\n",
    "                response = chain.invoke(query)\n",
    "                results.append({\n",
    "                    \"chunk_size\": chunk_size,\n",
    "                    \"chunk_overlap\": chunk_overlap,\n",
    "                    \"query\": query,\n",
    "                    \"response\": response\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"chunk_size\": chunk_size,\n",
    "                    \"chunk_overlap\": chunk_overlap,\n",
    "                    \"query\": query,\n",
    "                    \"response\": f\"Error: {e}\"\n",
    "                })\n",
    "\n",
    "# Step 5: Save results to Excel\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel(\"chunk_size_overlap_tests_2.xlsx\", index=False)\n",
    "print(\"Results saved to 'chunk_size_overlap_tests.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_okay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
